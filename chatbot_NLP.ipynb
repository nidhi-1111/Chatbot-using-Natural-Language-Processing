{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GF9RZV5urKYG",
    "outputId": "c10628ad-f746-4fd5-cd8c-7e5e6f901d46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nidhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nidhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPeN_fBcrTtu",
    "outputId": "3a5e45c8-62db-440e-f23e-fc4016b9fbf7"
   },
   "outputs": [],
   "source": [
    "f = open('DS.txt', 'r', errors = 'ignore')\n",
    "preprocessed_text = f.read()\n",
    "preprocessed_text = preprocessed_text.lower()\n",
    "sent_tokens = nltk.sent_tokenize(preprocessed_text)\n",
    "word_tokens = nltk.word_tokenize(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LqrYkdZpzRFD"
   },
   "outputs": [],
   "source": [
    "punctuations = dict((ord(punct), punct) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iQ2eXrFnv0Gz"
   },
   "outputs": [],
   "source": [
    "#preprocessing the text\n",
    "lem = nltk.stem.WordNetLemmatizer()\n",
    "def Ltokens(tokens):\n",
    "  return [lem.lemmatize(token) for token in tokens]\n",
    "punctuations = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def Lnorm(text):\n",
    "  return Ltokens(nltk.word_tokenize(text.lower().translate(punctuations)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bAdz4sbdyQ1g"
   },
   "outputs": [],
   "source": [
    "convo_starter = ('hello', 'hi', 'good morning', 'good afternoon', 'hey', 'ssup', \"what's up\", 'okay', 'got it', 'understood', 'understand')\n",
    "convo_res = ['yes', 'tell me', 'good', 'how can i help you?', 'ask me anything about data science']\n",
    "go_ahead = ['okay', 'got it', 'understood', 'understand']\n",
    "def start(sentence):\n",
    "  for word in sentence.split():\n",
    "    if word.lower() in convo_starter:\n",
    "      if word in go_ahead:\n",
    "        return 'Glad that I was able to, ask me next one!'\n",
    "      else:\n",
    "        return random.choice(convo_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sp2ucQTG0Ieg"
   },
   "outputs": [],
   "source": [
    " from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gIwNFh4ZVAea"
   },
   "outputs": [],
   "source": [
    "def reply(user_reply):\n",
    "  bot_reply = ''\n",
    "  tfIdfVectorizer=TfidfVectorizer(tokenizer = Lnorm, stop_words = 'english')\n",
    "  model = tfIdfVectorizer.fit_transform(sent_tokens)\n",
    "  #df = pd.DataFrame(model[0].T.todense(), index = tfIdfVectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "  #df = df.sort_values('TF-IDF', ascending=False)\n",
    "  val = cosine_similarity(model[-1], model)\n",
    "  idx = val.argsort()[0][-2]\n",
    "  flat = val.flatten()\n",
    "  flat.sort()\n",
    "  req_tfidf = flat[-2]\n",
    "  if(req_tfidf == 0):\n",
    "    bot_reply = bot_reply + \"I am sorry! I don't understand you, can you please try again\"\n",
    "    return bot_reply\n",
    "  else:\n",
    "    bot_reply = bot_reply + sent_tokens[idx]\n",
    "    return bot_reply\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfHKl6t-jEIZ",
    "outputId": "3b49db71-b94e-472d-f30e-d4eb0ca15439",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: My name is Sara, I'm here to help you with your data science queries. Shoot me your questions ;)\n",
      "Hi\n",
      "Bot:yes\n",
      "p-value\n",
      "Bot: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nidhi\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17. what is the significance of p-value?\n",
      "supervised machine learning\n",
      "Bot: 1. what are the differences between supervised and unsupervised learning?\n",
      "data cleaning\n",
      "Bot: when we're limiting or selecting the features, it's all about cleaning up the data coming in.\n",
      "feature extraction\n",
      "Bot: 33. what are the feature vectors?\n",
      "bye\n",
      "Bot: Bye, it was nice talking to you!\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"Bot: My name is Sara, I'm here to help you with your data science queries. Shoot me your questions ;)\")\n",
    "while(flag==True):\n",
    "  user_reply = input()\n",
    "  user_reply = user_reply.lower()\n",
    "  if(user_reply != 'bye'):\n",
    "    if(user_reply == 'thanks' or user_reply == 'thank you'):\n",
    "      flag = False\n",
    "      print(\"Bot: You're welcome anytime\")\n",
    "    else:\n",
    "      if(start(user_reply)!= None):\n",
    "        print(\"Bot:\" + start(user_reply))\n",
    "      else:\n",
    "        sent_tokens.append(user_reply)\n",
    "        word_tokens = word_tokens + nltk.word_tokenize(user_reply)\n",
    "        final_words = list(set(word_tokens))\n",
    "        print(\"Bot: \", end = \"\")\n",
    "        print(reply(user_reply))\n",
    "        sent_tokens.remove(user_reply)\n",
    "  else:\n",
    "    flag = False\n",
    "    print(\"Bot: Bye, it was nice talking to you!\")      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
